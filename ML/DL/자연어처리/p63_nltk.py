# -*- coding: utf-8 -*-
"""p63.nltk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_iKGDQJQnlKyRg0GDG-c7M_2HwbY3N6z
"""

import nltk
nltk.download('punkt')      # '/root/nltk_data/tokenizers'
nltk.download('stopwords')  # 불용어 목록

from nltk.tokenize import word_tokenize

sentence = """
Natural language processing (NLP) is a subfield of computer science, information engineering, 
and artificial intelligence concerned with the interactions between computers and human (natural) languages, 
in particular how to program computers to process and analyze large amounts of natural language data.
"""

print(word_tokenize(sentence))

from nltk.tokenize import sent_tokenize

paragraph = """
Natural language processing (NLP) is a subfield of computer science, information engineering, 
and artificial intelligence concerned with the interactions between computers and human (natural) languages, 
in particular how to program computers to process and analyze large amounts of natural language data. 
Challenges in natural language processing frequently involve speech recognition, natural language understanding, 
and natural language generation.
"""

print(sent_tokenize(paragraph))

# 텍스트에서 단어 분리
word_tokens = [word_tokenize(x) for x in sent_tokenize(paragraph)]
word_tokens[1]

# stop words 제거
stopwords = nltk.corpus.stopwords.words('english')  # 등록된 stop word
stopwords.append(',')
stopwords.append(')')
stopwords.append('(')

print(stopwords)

# 불용어를 제거한다.
all_tokens = []
for sent in word_tokens:
    all_tokens.append([word for word in sent if word.lower() not in stopwords])
all_tokens

# Stemming
from nltk.stem import LancasterStemmer
stemmer = LancasterStemmer()

for word in ['working', 'works', 'worked']:
    print(stemmer.stem(word))

tokens_stem = []
for sent in all_tokens:
    sentence = []
    for word in sent:
        sentence.append(stemmer.stem(word))
    tokens_stem.append(sentence)
tokens_stem

# Lemmatization
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

lemma = WordNetLemmatizer()
for word in ['working', 'works', 'worked']:
    print(lemma.lemmatize(word, 'v'))

for word in ['happier', 'happiest']:
    print(lemma.lemmatize(word, 'a'))

# 사전 (vocabulary) 생성
word2idx = {}
n_idx = 0
for sent in all_tokens:
    for word in sent:
        if word.lower() not in word2idx:
            word2idx[word.lower()] = n_idx
            n_idx += 1
idx2word = {v:k for k, v in word2idx.items()}

word2idx

# text를 사전의 index로 표현
text_idx = []
for sent in all_tokens:
    sent_idx = []
    for word in sent:
        sent_idx.append(word2idx[word.lower()])
    text_idx.append(sent_idx)
print(text_idx[0])

# text_idx를 다시 단어로 표시
text = []
for sent_idx in text_idx:
    sent = []
    for word_idx in sent_idx:
        sent.append(idx2word[word_idx])
    text.append(sent)
print(text[0])

