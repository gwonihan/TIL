{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6b291c",
   "metadata": {},
   "source": [
    "# 16. 순환 신경망으로 순차 데이터 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263cbfee",
   "metadata": {},
   "source": [
    "## 16.1 순차 데이터 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfec653",
   "metadata": {},
   "source": [
    "## 16.2 시퀀스 모델링을 위한 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3440ba",
   "metadata": {},
   "source": [
    "### 16.2.3 은닉 순환과 출력 순환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0c0168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh 크기: (5, 2)\n",
      "W_oo 크기: (2, 2)\n",
      "b_h 크기: (2,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    "rnn_layer = tf.keras.layers.SimpleRNN(\n",
    "    units=2, use_bias=True, return_sequences=True)\n",
    "rnn_layer.build(input_shape=(None, None, 5))\n",
    "w_xh, w_oo, b_h = rnn_layer.weights\n",
    "print('W_xh 크기:', w_xh.shape)\n",
    "print('W_oo 크기:', w_oo.shape)\n",
    "print('b_h 크기:', b_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26f90e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타임 스텝 0 =>\n",
      "  입력             : [[1. 1. 1. 1. 1.]]\n",
      "  은닉             : [[-0.69677734 -0.8956299 ]]\n",
      "  출력(수동)       : [[-0.6023183  -0.71416336]]\n",
      "  SimpleRNN 출력   : [-0.6023183  -0.71416336]\n",
      "\n",
      "타임 스텝 1 =>\n",
      "  입력             : [[2. 2. 2. 2. 2.]]\n",
      "  은닉             : [[-1.3935547 -1.7912598]]\n",
      "  출력(수동)       : [[-0.94736236 -0.9896641 ]]\n",
      "  SimpleRNN 출력   : [-0.94736236 -0.9896641 ]\n",
      "\n",
      "타임 스텝 2 =>\n",
      "  입력             : [[3. 3. 3. 3. 3.]]\n",
      "  은닉             : [[-2.090332  -2.6868896]]\n",
      "  출력(수동)       : [[-0.9921783  -0.99914104]]\n",
      "  SimpleRNN 출력   : [-0.9921783  -0.99914104]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq = tf.convert_to_tensor([[1.0]*5, [2.0]*5, [3.0]*5], dtype=tf.float32)\n",
    "## SimpleRNN의 출력\n",
    "output = rnn_layer(tf.reshape(x_seq, shape=(1, 3, 5)))\n",
    "\n",
    "## 수동으로 출력 계산하기\n",
    "out_man = []\n",
    "for t in range(len(x_seq)):\n",
    "    xt = tf.reshape(x_seq[t], (1, 5))\n",
    "    print('타임 스텝 {} =>'.format(t))\n",
    "    print('  입력             :', xt.numpy())\n",
    "    \n",
    "    ht = tf.matmul(xt, w_xh) + b_h\n",
    "    print('  은닉             :', ht.numpy())\n",
    "    \n",
    "    if t>0:\n",
    "        prev_o = out_man[t-1]\n",
    "    else:\n",
    "        prev_o = tf.zeros(shape=(ht.shape))\n",
    "    ot = ht + tf.matmul(prev_o, w_oo)\n",
    "    ot = tf.math.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('  출력(수동)       :', ot.numpy())\n",
    "    print('  SimpleRNN 출력   :'.format(t), output[0][t].numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95cd554",
   "metadata": {},
   "source": [
    "## 16.3 텐서플로로 시퀀스 모델링을 위한 RNN 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e93f44",
   "metadata": {},
   "source": [
    "### 16.3.1 첫 번째 프로젝트:IMDb 영화 리뷰의 감성 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79694dd0",
   "metadata": {},
   "source": [
    "영화 리뷰 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c393be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fde8515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'In 1974, the teenager Martha Moxley (Maggie Grace)' 1\n",
      "b'OK... so... I really like Kris Kristofferson and h' 0\n",
      "b'***SPOILER*** Do not read this, if you think about' 0\n"
     ]
    }
   ],
   "source": [
    "## 1단계 : 데이터셋 만들기\n",
    "target = df.pop('sentiment')\n",
    "ds_raw = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
    "\n",
    "## 확인:\n",
    "for ex in ds_raw.take(3):\n",
    "    tf.print(ex[0].numpy()[0][:50], ex[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9744130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일정한 결과값을 얻기 위한 seed값 설정\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "ds_raw = ds_raw.shuffle(50000, reshuffle_each_iteration=False)\n",
    "ds_raw_test = ds_raw.take(25000)\n",
    "ds_raw_train_valid = ds_raw.skip(25000)\n",
    "ds_raw_train = ds_raw_train_valid.take(20000)\n",
    "ds_raw_valid = ds_raw_train_valid.skip(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da45f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전 크기: 87007\n"
     ]
    }
   ],
   "source": [
    "## 2단계: 고유 토큰(단어) 찾기\n",
    "from collections import Counter\n",
    "tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "token_counts = Counter()\n",
    "for example in ds_raw_train:\n",
    "    tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print('어휘 사전 크기:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9943a59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[232, 9, 270, 1123]\n"
     ]
    }
   ],
   "source": [
    "## 3단계: 고유 토큰을 정수로 인코딩하기\n",
    "encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
    "example_str = 'This is an example!'\n",
    "print(encoder.encode(example_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba742f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시퀀스 길이 (24,)\n",
      "시퀀스 길이 (179,)\n",
      "시퀀스 길이 (262,)\n",
      "시퀀스 길이 (535,)\n",
      "시퀀스 길이 (130,)\n"
     ]
    }
   ],
   "source": [
    "## 3-A단계: 변환을 위한 함수 정의\n",
    "def encode(text_tensor, label):\n",
    "    text = text_tensor.numpy()[0]\n",
    "    encoded_text = encoder.encode(text)\n",
    "    return encoded_text, label\n",
    "\n",
    "## 3-B단계: 함수를 TF 연산으로 변환하기\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "ds_train = ds_raw_train.map(encode_map_fn)\n",
    "ds_valid = ds_raw_valid.map(encode_map_fn)\n",
    "ds_test = ds_raw_test.map(encode_map_fn)\n",
    "\n",
    "#샘플의 크기 확인하기:\n",
    "tf.random.set_seed(1)\n",
    "for example in ds_train.shuffle(1000).take(5):\n",
    "    print('시퀀스 길이', example[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2944ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개별 샘플 크기: (119,)\n",
      "개별 샘플 크기: (688,)\n",
      "개별 샘플 크기: (308,)\n",
      "개별 샘플 크기: (204,)\n",
      "개별 샘플 크기: (326,)\n",
      "개별 샘플 크기: (240,)\n",
      "개별 샘플 크기: (127,)\n",
      "개별 샘플 크기: (453,)\n"
     ]
    }
   ],
   "source": [
    "## 일부 데이터 추출하기\n",
    "ds_subset = ds_train.take(8)\n",
    "for example in ds_subset:\n",
    "    print('개별 샘플 크기:', example[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a40585b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 차원: (4, 688)\n",
      "배치 차원: (4, 453)\n"
     ]
    }
   ],
   "source": [
    "## 배치 데이터 만들기\n",
    "ds_batched = ds_subset.padded_batch(\n",
    "             4, padded_shapes=([-1], []))\n",
    "for batch in ds_batched:\n",
    "    print('배치 차원:', batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68b62c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ds_train.padded_batch(32, padded_shapes=([-1], []))\n",
    "valid_data = ds_valid.padded_batch(32, padded_shapes=([-1], []))\n",
    "test_data = ds_test.padded_batch(32, padded_shapes=([-1], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2bd24eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed-layer (Embedding)     (None, 20, 6)             600       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 600\n",
      "Trainable params: 600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(input_dim=100, output_dim=6, input_length=20, name='embed-layer'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84b4548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          32000     \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, None, 32)          2080      \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,193\n",
      "Trainable params: 36,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ef777e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed-layer (Embedding)     (None, None, 20)          1740180   \n",
      "                                                                 \n",
      " bidir-lstm (Bidirectional)  (None, 128)               43520     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,792,021\n",
      "Trainable params: 1,792,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 46s 68ms/step - loss: 0.5155 - accuracy: 0.7441 - val_loss: 0.3775 - val_accuracy: 0.8472\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.2605 - accuracy: 0.9006 - val_loss: 0.3898 - val_accuracy: 0.8402\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 44s 71ms/step - loss: 0.1509 - accuracy: 0.9474 - val_loss: 0.4603 - val_accuracy: 0.8430\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.1209 - accuracy: 0.9589 - val_loss: 0.5269 - val_accuracy: 0.8380\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 43s 69ms/step - loss: 0.0661 - accuracy: 0.9790 - val_loss: 0.5465 - val_accuracy: 0.8188\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 43s 69ms/step - loss: 0.0639 - accuracy: 0.9808 - val_loss: 0.5987 - val_accuracy: 0.8328\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 44s 71ms/step - loss: 0.0410 - accuracy: 0.9876 - val_loss: 0.6815 - val_accuracy: 0.8372\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 44s 71ms/step - loss: 0.0531 - accuracy: 0.9857 - val_loss: 0.8040 - val_accuracy: 0.8362\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 42s 68ms/step - loss: 0.0324 - accuracy: 0.9906 - val_loss: 0.6730 - val_accuracy: 0.8434\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 44s 70ms/step - loss: 0.0183 - accuracy: 0.9951 - val_loss: 0.8002 - val_accuracy: 0.8382\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7867 - accuracy: 0.8424\n",
      "테스트 정확도: 84.24%\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 20\n",
    "vocab_size = len(token_counts) + 2\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "## 모델 만들기\n",
    "bi_lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        name='embed-layer'),\n",
    "    \n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, name='lstm-layer'),\n",
    "        name='bidir-lstm'),\n",
    "    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bi_lstm_model.summary()\n",
    "\n",
    "## 컴파일과 훈련\n",
    "bi_lstm_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy'])\n",
    "history = bi_lstm_model.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=10)\n",
    "\n",
    "## 테스트 데이터에서 평가\n",
    "test_results = bi_lstm_model.evaluate(test_data)\n",
    "print('테스트 정확도: {:.2f}%'.format(test_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38bd2fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def preprocess_datasets(\n",
    "    ds_raw_train,\n",
    "    ds_raw_valid,\n",
    "    ds_raw_test,\n",
    "    max_seq_length=None,\n",
    "    batch_size=32):\n",
    "    \n",
    "    ## 1단계: (데이터셋 만들기 이미 완료)\n",
    "    ## 2단계: 고유 토큰 찾기\n",
    "    tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "    token_counts = Counter()\n",
    "    \n",
    "    for example in ds_raw_train:\n",
    "        tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
    "        if max_seq_length is not None:\n",
    "            tokens = tokens[-max_seq_length:]\n",
    "        token_counts.update(tokens)\n",
    "        \n",
    "    print('어휘 사전 크기:', len(token_counts))\n",
    "    \n",
    "    ## 3단계: 텍스트 인코딩하기\n",
    "    encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
    "    \n",
    "    def encode(text_tensor, label):\n",
    "        text = text_tensor.numpy()[0]\n",
    "        encoded_text = encoder.encode(text)\n",
    "        if max_seq_length is not None:\n",
    "            encoded_text = encoded_text[-max_seq_length:]\n",
    "        return encoded_text, label\n",
    "    \n",
    "    def encode_map_fn(text, label):\n",
    "        return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "    \n",
    "    ds_train = ds_raw_train.map(encode_map_fn)\n",
    "    ds_valid = ds_raw_valid.map(encode_map_fn)\n",
    "    ds_test = ds_raw_test.map(encode_map_fn)\n",
    "    \n",
    "    ## 4단계: 배치 데이터 만들기\n",
    "    train_data = ds_train.padded_batch(batch_size, padded_shapes=([-1], []))\n",
    "    valid_data = ds_valid.padded_batch(batch_size, padded_shapes=([-1], []))\n",
    "    test_data = ds_test.padded_batch(batch_size, padded_shapes=([-1], []))\n",
    "    \n",
    "    return (train_data, valid_data, test_data, len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaadf08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "def build_rnn_model(embedding_dim, vocab_size, recurrent_type='SimpleRNN', n_recurrent_units=64,\n",
    "                    n_recurrent_layers=1, bidirectional=True):\n",
    "    tf.random.set_seed(1)\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, name='embed-layer'))\n",
    "    \n",
    "    for i in range(n_recurrent_layers):\n",
    "        return_sequences = (i < n_recurrent_layers -1)\n",
    "        \n",
    "        if recurrent_type == 'SimpleRNN':\n",
    "            recurrent_layer = SimpleRNN(\n",
    "                units=n_recurrent_units, return_sequences=return_sequences,\n",
    "                name='simplernn-layer-{}'.format(i))\n",
    "        elif recurrent_type == 'LSTM':\n",
    "            recurrent_layer = LSTM(\n",
    "                units=n_recurrent_units, return_sequences=return_sequences,\n",
    "                name='lstm-layer-{}'.format(i))\n",
    "        elif recurrent_type == 'GRU':\n",
    "            recurrent_layer = GRU(\n",
    "                units=n_recurrent_units, return_sequences=return_sequences,\n",
    "                name='gru-layer-{}'.format(i))\n",
    "        \n",
    "        if bidirectional:\n",
    "            recurrent_layer = Bidirectional(\n",
    "                recurrent_layer, name='bidir-' + recurrent_layer.name)\n",
    "            \n",
    "        model.add(recurrent_layer)\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc1cceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전 크기: 58063\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed-layer (Embedding)     (None, None, 20)          1161300   \n",
      "                                                                 \n",
      " bidir-simplernn-layer-0 (Bi  (None, 128)              10880     \n",
      " directional)                                                    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,180,501\n",
      "Trainable params: 1,180,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "embedding_dim = 20\n",
    "max_seq_length = 100\n",
    "train_data, valid_data, test_data, n = preprocess_datasets(\n",
    "    ds_raw_train, ds_raw_valid, ds_raw_test, max_seq_length=max_seq_length, batch_size=batch_size)\n",
    "\n",
    "vocab_size = n + 2\n",
    "rnn_model = build_rnn_model(embedding_dim, vocab_size, recurrent_type='SimpleRNN',\n",
    "                            n_recurrent_units=64, n_recurrent_layers=1, bidirectional=True)\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa5eae97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 107s 170ms/step - loss: 0.6717 - accuracy: 0.5530 - val_loss: 0.5717 - val_accuracy: 0.6962\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 109s 174ms/step - loss: 0.5074 - accuracy: 0.7379 - val_loss: 0.5047 - val_accuracy: 0.7494\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 113s 180ms/step - loss: 0.2387 - accuracy: 0.9020 - val_loss: 0.5078 - val_accuracy: 0.8068\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 112s 178ms/step - loss: 0.0887 - accuracy: 0.9694 - val_loss: 0.7901 - val_accuracy: 0.8052\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 128s 204ms/step - loss: 0.0555 - accuracy: 0.9790 - val_loss: 1.0364 - val_accuracy: 0.8056\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 122s 196ms/step - loss: 0.0313 - accuracy: 0.9890 - val_loss: 1.0335 - val_accuracy: 0.8026\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 125s 200ms/step - loss: 0.0227 - accuracy: 0.9919 - val_loss: 1.0681 - val_accuracy: 0.7842\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 126s 201ms/step - loss: 0.0251 - accuracy: 0.9909 - val_loss: 1.2074 - val_accuracy: 0.6784\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 123s 197ms/step - loss: 0.0189 - accuracy: 0.9938 - val_loss: 1.1750 - val_accuracy: 0.7528\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 124s 198ms/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 1.2713 - val_accuracy: 0.7654\n"
     ]
    }
   ],
   "source": [
    "rnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(\n",
    "        from_logits=False), metrics=['accuracy'])\n",
    "\n",
    "history = rnn_model.fit(\n",
    "    train_data, validation_data=valid_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc4ec10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 35s 44ms/step - loss: 1.2945 - accuracy: 0.7610\n",
      "테스트 정확도: 76.10%\n"
     ]
    }
   ],
   "source": [
    "results = rnn_model.evaluate(test_data)\n",
    "print('테스트 정확도: {:.2f}%'.format(results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a756ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7899578670518958593\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5769199616\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6458989066248694261\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "import PIL\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c4952",
   "metadata": {},
   "source": [
    "### 16.3.2 두 번째 프로젝트: 텐서플로로 글자 단위 언어 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a26ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 길이: 1112350\n",
      "고유한 문자: 80\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "## 텍스트 읽고 전처리하기\n",
    "with open('1268-0.txt', 'r', encoding='UTF8') as fp:\n",
    "    text=fp.read()\n",
    "\n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')\n",
    "text = text[start_indx:end_indx]\n",
    "char_set = set(text)\n",
    "print('전체 길이:', len(text))\n",
    "print('고유한 문자:', len(char_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c76df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코딩된 텍스트 크기: (1112350,)\n",
      "THE MYSTERIOUS      == 인코딩 ==> [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
      "[33 43 36 25 38 28]     == 디코딩 ==> ISLAND\n"
     ]
    }
   ],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "\n",
    "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
    "print('인코딩된 텍스트 크기:', text_encoded.shape)\n",
    "print(text[:15], '    == 인코딩 ==>', text_encoded[:15])\n",
    "print(text_encoded[15:21], '    == 디코딩 ==>', ''.join(char_array[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b46279bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 -> T\n",
      "32 -> H\n",
      "29 -> E\n",
      "1 ->  \n",
      "37 -> M\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
    "for ex in ds_text_encoded.take(5):\n",
    "    print('{} -> {}'.format(ex.numpy(), char_array[ex.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d7a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "ds_chunks = ds_text_encoded.batch(chunk_size, drop_remainder=True)\n",
    "\n",
    "## x & y를 나누기 위한 함수를 정의한다.\n",
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]\n",
    "    target_seq = chunk[1:]\n",
    "    return input_seq, target_seq\n",
    "\n",
    "ds_sequences = ds_chunks.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0071141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
      "타깃 (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "\n",
      "입력 (x): ' Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n'\n",
      "타깃 (y): 'Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n\\n'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in ds_sequences.take(2):\n",
    "    print('입력 (x):', repr(''.join(char_array[example[0].numpy()])))\n",
    "    print('타깃 (y):', repr(''.join(char_array[example[1].numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65258859",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "ds = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceb66411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         20480     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 512)         1574912   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 80)          41040     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,636,432\n",
      "Trainable params: 1,636,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units,\n",
    "            return_sequences=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "## 매개변수 설정\n",
    "charset_size = len(char_array)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "tf.random.set_seed(1)\n",
    "model = build_model(\n",
    "    vocab_size=charset_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "300efd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "424/424 [==============================] - 7s 9ms/step - loss: 2.3278\n",
      "Epoch 2/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.7456\n",
      "Epoch 3/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.5373\n",
      "Epoch 4/20\n",
      "424/424 [==============================] - 4s 10ms/step - loss: 1.4216\n",
      "Epoch 5/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.3491\n",
      "Epoch 6/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.2981\n",
      "Epoch 7/20\n",
      "424/424 [==============================] - 4s 10ms/step - loss: 1.2598\n",
      "Epoch 8/20\n",
      "424/424 [==============================] - 5s 11ms/step - loss: 1.2295\n",
      "Epoch 9/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.2041\n",
      "Epoch 10/20\n",
      "424/424 [==============================] - 4s 10ms/step - loss: 1.1820\n",
      "Epoch 11/20\n",
      "424/424 [==============================] - 5s 11ms/step - loss: 1.1617\n",
      "Epoch 12/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.1437\n",
      "Epoch 13/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.1268\n",
      "Epoch 14/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.1111\n",
      "Epoch 15/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.0967\n",
      "Epoch 16/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.0824\n",
      "Epoch 17/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.0688\n",
      "Epoch 18/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.0553\n",
      "Epoch 19/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.0424\n",
      "Epoch 20/20\n",
      "424/424 [==============================] - 4s 9ms/step - loss: 1.0298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23e86836610>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.fit(ds, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33ccf095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "확률: [0.33333334 0.33333334 0.33333334]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "logits = [[1.0, 1.0, 1.0]]\n",
    "print('확률:', tf.math.softmax(logits).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8170d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1, 2, 0, 1, 0, 1, 1, 2, 1, 1]], dtype=int64)\r\n"
     ]
    }
   ],
   "source": [
    "samples = tf.random.categorical(\n",
    "    logits=logits, num_samples=10)\n",
    "tf.print(samples.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44d9e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "확률:  [0.10650698 0.10650698 0.78698605]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "logits = [[1.0, 1.0, 3.0]]\n",
    "print('확률: ', tf.math.softmax(logits).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce7406fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2, 2, 0, 2, 2, 2, 2, 2, 1, 2]], dtype=int64)\r\n"
     ]
    }
   ],
   "source": [
    "samples = tf.random.categorical(\n",
    "    logits=logits, num_samples=10)\n",
    "tf.print(samples.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f66017e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, starting_str, len_generated_text=500, max_input_length=40, scale_factor=1.0):\n",
    "    encoded_input = [char2int[s] for s in starting_str]\n",
    "    encoded_input = tf.reshape(encoded_input, (1, -1))\n",
    "    \n",
    "    generated_str = starting_str\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(len_generated_text):\n",
    "        logits = model(encoded_input)\n",
    "        logits = tf.squeeze(logits, 0)\n",
    "        \n",
    "        scaled_logits = logits * scale_factor\n",
    "        new_char_indx = tf.random.categorical(\n",
    "            scaled_logits, num_samples=1)\n",
    "        \n",
    "        new_char_indx = tf.squeeze(new_char_indx)[-1].numpy()\n",
    "        \n",
    "        generated_str += str(char_array[new_char_indx])\n",
    "        \n",
    "        new_char_indx = tf.expand_dims([new_char_indx], 0)\n",
    "        encoded_input = tf.concat([encoded_input, new_char_indx], axis=1)\n",
    "        encoded_input = encoded_input[:, -max_input_length:]\n",
    "        \n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f76695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was extended on the reporter.\n",
      "\n",
      "“Spored up, a ciff making a minute the promontory, the rope had more trade, enther at the state of the convicts\n",
      "shore, Tabor Island indeed,” answered Herbert.\n",
      "\n",
      "“No, captainly,” replied Neb, “you did not sufficient soil to establish ourselves of the lake at the five or freeze, let us we likely to conden of\n",
      "elects with\n",
      "its splude\n",
      "which is\n",
      "extremely pieces.\n",
      "\n",
      "Of abyssible daily fever did not more any emergency. The cart was a cold with the rooms, although\n",
      "not walking \n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "print(sample(model, starting_str='The island'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81a0e9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스케일 조정 전의 확률: [0.10650698 0.10650698 0.78698604]\n",
      "0.5배 조정 후 확률: [0.21194156 0.21194156 0.57611688]\n",
      "0.1배 조정 후 확률: [0.31042377 0.31042377 0.37915245]\n"
     ]
    }
   ],
   "source": [
    "logits = np.array([[1.0, 1.0, 3.0]])\n",
    "print('스케일 조정 전의 확률:', tf.math.softmax(logits).numpy()[0])\n",
    "print('0.5배 조정 후 확률:', tf.math.softmax(0.5*logits).numpy()[0])\n",
    "print('0.1배 조정 후 확률:', tf.math.softmax(0.1*logits).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba1b6cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was extended on the road to the mouth of the coast when the mark for two miles from the extreme voice. The engineer was heard, and then, in the corral and an abundance of each other.\n",
      "\n",
      "It was not to be felt. The shade\n",
      "of Prospect Heights and the sea, the sea between the two country he could distinguish a double bain almost more brought be suitable.\n",
      "\n",
      "“And yet Ayrton! I will be able to remain standing in a beautiful trees. The engineer was very darked as the terrible danger in the right bank of th\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "print(sample(model, starting_str='The island', scale_factor=2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "297999b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was egbleer fox togetray? go Heave, Herbert! ciff master to\n",
      "me, Harding,--“To casctobary?” added; cerrud to wohes\n",
      "abonavh tak;\n",
      "the mosn, open,\n",
      "sin heat, evaxisyen ocerrasieas, mingodesion, projeeved\n",
      "up,\n",
      "listeng again\n",
      "barquiticy of\n",
      "ocions delb.\n",
      " Top, this inglete. Hadd Herb,! Then,\n",
      "for, indedicalent free! tullor 23 molmidutes; cother-glee-?”\n",
      "said Peninsuland-Afficatily over-tinking my lib. He had cast\n",
      "illyful-promolt of the ansle. He further. Lalts\n",
      "spreaked, with us. Towarded Falrs?\n",
      "To Juty 9uri\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "print(sample(model, starting_str='The island', scale_factor=0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
