# -*- coding: utf-8 -*-
"""KFold(iris).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qFxLB3D-6iA9h0PtDyQoHbK9KeXjEAce
"""

# K-Fold cross validation 기능 연습
import numpy as np
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold

iris = load_iris()

x_train, x_test, y_train, y_test = train_test_split(iris['data'], iris['target'], test_size = 0.2)

# 5-fold를 구성한다.
cv = KFold(n_splits = 5, shuffle=True)

depth_acc = []  # depth 별 정확도 리스트
for depth in range(1, 20):
    model = DecisionTreeClassifier(max_depth = depth)

    fold_acc = [] # k-fold 별 정확도 리스트
    for i, (tx, vx) in enumerate(cv.split(x_train)):
        # 학습용, 평가용 데이터
        xf_train, xf_eval = x_train[tx], x_train[vx]
        yf_train, yf_eval = y_train[tx], y_train[vx]

        # 학습
        model.fit(xf_train, yf_train)

        # 평가
        fold_acc.append(model.score(xf_eval, yf_eval))

    print("depth-{:2d} : fold acculate = {}".format(depth, np.round(fold_acc, 3)), end="")

    # fold 별 정확도 평균을 저장한다 = depth 별 정확도
    depth_acc.append(np.mean(fold_acc))
    print(" --> 평균 = {:.4f}".format(depth_acc[-1]))

# 최적 depth로 model를 최종 학습하고, 시험 데이터로 성능을 평가한다.
opt_depth = np.argmax(depth_acc) + 1

model = DecisionTreeClassifier(max_depth = opt_depth)
model.fit(x_train, y_train)

print("\n최적 depth = {}".format(opt_depth))
print("시험 데이터 정확도 = {:.4f}".format(model.score(x_test, y_test)))

