# -*- coding: utf-8 -*-
"""7-13.transformer(train_enc_dec).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o_s6TGYkaOyk-6KdVkxmVZVNWkFtgYPF
"""

# Commented out IPython magic to ensure Python compatibility.
# Transformer ChatBot : 학습 모듈
# 관련 논문 : Ashish Vaswani, et, al. 2017, Attention Is All You Need
#
# transformer 라이브러리 출처 : https://github.com/suyash/transformer
# 챗봇 제작자 : 2020.06.07, 조성현 (blog.naver.com/chunjein)
# copyright: SNS 등에 공개할 때는 출처에 제작자를 명시해 주시기 바랍니다.
# -----------------------------------------------------------------------
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras import optimizers
import tensorflow.keras.backend as K
import matplotlib.pyplot as plt
import pickle

# %cd '/content/drive/MyDrive/삼성멀캠(2022)/3.자연어처리/6.챗봇'
from transformer import Encoder, Decoder, PaddingMask, PaddingAndLookaheadMask

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/MyDrive/Colab Notebooks'

# 단어 목록 dict를 읽어온다.
with open('data/chatbot_voc.pkl', 'rb') as f:
    word2idx,  idx2word = pickle.load(f)
    
# 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 읽어온다.
with open('data/chatbot_train.pkl', 'rb') as f:
    trainXE, trainXD, trainYD, _, _ = pickle.load(f)

MODEL_PATH = 'data/transformer_model.h5'
VOCAB_SIZE = len(word2idx)
MAX_LEN = 15
LOAD_MODEL = True

# Model
# -----
K.clear_session()

# 인코더 입력 (source). 입력 문장의 길이는 MAX_LEN으로 고정돼 있다.
src = Input(batch_shape = (None, MAX_LEN), dtype="int32", name="src")

# 디코더 입력 (target). 학습할 때는 입력 문장의 길이가 MAX_LEN으로 고정돼 있다. (Teacher forcing)
# chatting 모듈에서 다음 단어를 예측할 때는 마지막 차원을 None으로 설정 해야한다.
tar = Input(batch_shape = (None, MAX_LEN), dtype="int32", name="tar")

# Encoder
# -------
padding_mask = PaddingMask()(src)
encoder = Encoder(num_layers=4, d_model=128, num_heads=8, d_ff=512, vocab_size=VOCAB_SIZE, dropout_rate=0.1)
enc_output, _ = encoder(src, padding_mask)

# Decoder
# -------
lookahead_mask = PaddingAndLookaheadMask()(tar)
decoder = Decoder(num_layers=4, d_model=128, num_heads=8, d_ff=512, vocab_size=VOCAB_SIZE, dropout_rate=0.1)
dec_output, _, _ = decoder(tar, enc_output, lookahead_mask, padding_mask)

# Final output
final_output = Dense(VOCAB_SIZE, activation='softmax')(dec_output)

model = Model(inputs=[src, tar], outputs=final_output)
model.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy')

if LOAD_MODEL:
    model.load_weights(MODEL_PATH)

model.summary()

# 학습 (teacher forcing)
# ----------------------
hist = model.fit([trainXE, trainXD], trainYD, batch_size = 512, epochs=100, shuffle=True)

# 학습 결과를 저장한다
model.save(MODEL_PATH)

# Loss history를 그린다
plt.plot(hist.history['loss'], label='Train loss')
plt.legend()
plt.title("Loss history")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

