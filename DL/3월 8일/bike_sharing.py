# -*- coding: utf-8 -*-
"""8.bike_sharing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BmYV8Z5fELvuUJpmEu4Tktvl0Sr_vIVJ
"""

# 자전거 대여 횟수 추정. Bike sharing demend
import pandas as pd
import numpy as np
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt

MY_PATH = '/content/drive/My Drive/Colab Notebooks/data/'
df = pd.read_csv(MY_PATH + 'bike_train.csv')
# df.info()

df.head()

# datetime을 년, 월, 일, 시간 컬럼으로 분리
df['datetime'] = df['datetime'].apply(pd.to_datetime)
df['year'] = df['datetime'].apply(lambda x: x.year)
df['month'] = df['datetime'].apply(lambda x: x.month)
df['day'] = df['datetime'].apply(lambda x: x.day)
df['hour'] = df['datetime'].apply(lambda x: x.hour)

# 불필요한 컬럼 삭제
df = df.drop(['datetime', 'casual', 'registered'], axis=1)

# 학습 데이터. Feature data를 두 부분으로 나눠서 처리.
ohe_feat = ['year', 'month', 'day', 'hour', 'holiday', 'workingday', 'season', 'weather'] # one-hot 변환 대상
std_feat = ['temp', 'atemp', 'humidity', 'windspeed']  # normalization 대상

x_feat1 = np.array(pd.get_dummies(df[ohe_feat], columns=ohe_feat))
x_feat2 = np.array(df[std_feat])
y_target = np.array(df['count']).reshape(-1, 1)

# 학습 데이터와 시험 데이터 분리
x_train1, x_test1, x_train2, x_test2, y_train, y_test = train_test_split(
    x_feat1, x_feat2, y_target, test_size=0.2)
x_train1.shape, x_test1.shape, x_train2.shape, x_test2.shape, y_train.shape, y_test.shape

# 데이터 표준화
# feature data 표준화
xf_mu = x_train2.mean(axis=0)
xf_sd = x_train2.std(axis=0)

x_train2_std = (x_train2 - xf_mu) / xf_sd
x_test2_std = (x_test2 - xf_mu) / xf_sd      # train data의 평균, 표준편차 사용

# target data 표준화
yt_mu = y_train.mean()
yt_sd = y_train.std()

y_train = (y_train - yt_mu) / yt_sd
y_test = (y_test - yt_mu) / yt_sd

# 분리된 2개의 feature data를 하나로 합친다.
x_train = np.hstack([x_train1, x_train2_std])
x_test = np.hstack([x_test1, x_test2_std])
x_train.shape, x_test.shape, y_train.shape, y_test.shape

n_hLayer = 2
n_hNeuron = 128

xInput = Input(batch_shape=(None, x_train.shape[1]))
hLayer = Dense(n_hNeuron, activation='relu')(xInput)
hLayer = Dropout(rate = 0.2)(hLayer)

for i in range(n_hLayer - 1):
    hLayer = Dense(n_hNeuron, activation='relu')(hLayer)
    hLayer = Dropout(rate = 0.2)(hLayer)
    
yOutput = Dense(1)(hLayer)

model = Model(xInput, yOutput)
model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))
model.summary()

hist = model.fit(x_train, y_train,
                 batch_size = 256,
                 epochs = 100,
                 validation_data = (x_test, y_test))

# loss 확인
plt.plot(hist.history['loss'], label='train')
plt.plot(hist.history['val_loss'], label='test')
plt.legend()
plt.show()

y_pred = model.predict(x_test)

y_pred_org = (y_pred * yt_sd + yt_mu).reshape(-1,)  # 표준화 이전 값으로 복원한다.
y_test_org = (y_test * yt_sd + yt_mu).reshape(-1,)

# 추정결과를 육안으로 확인한다.
result = pd.DataFrame({'y_test': y_test_org, 'y_pred': y_pred_org})
result.head()

# 추정치가 음수가 나온 경우 육안 확인
result[result['y_pred'] < 0]

# 추정치가 음수가 나온 경우는 y_train의 최솟값으로 치환
y_train_org = y_train * yt_sd + yt_mu
idx = np.where(y_pred_org < 0)[0]
y_pred_org[idx] = y_train_org.min()

# 추정결과를 육안으로 확인한다.
result = pd.DataFrame({'y_test': y_test_org, 'y_pred': y_pred_org})
result.head()

# 추정 결과를 산포도로 확인한다.
plt.scatter(y_test_org, y_pred_org, marker='o', s=10, c='red')
plt.xlabel('y_test')
plt.ylabel('y_pred')
plt.show()

print('R2 score = {:.4f}'.format(r2_score(y_test_org, y_pred_org)))

