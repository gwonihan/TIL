# -*- coding: utf-8 -*-
"""4-3.SkipGram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lecTMG8iDtBad_zuJvu7CuYwdicYsCja
"""

import numpy as np
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from nltk.stem import LancasterStemmer
from tensorflow.keras.layers import Input, Embedding, Dense
from tensorflow.keras.models import Model
from sklearn.metrics.pairwise import cosine_similarity

nltk.download('punkt')
nltk.download('gutenberg')
nltk.download('stopwords')

# 영문 소설 18개를 읽어와서 전처리를 수행한다.
n = 18
stemmer = LancasterStemmer()
stopwords = nltk.corpus.stopwords.words('english')
stopwords.extend(['and', 'but', 'the', 'for', 'would', 'shall'])

sent_stem = []
files = nltk.corpus.gutenberg.fileids()
for i, text_id in enumerate(files[:n]):
    text = nltk.corpus.gutenberg.raw(text_id)
    sentences = nltk.sent_tokenize(text)

    # 각 단어에 Lancaster stemmer를 적용한다.
    for sentence in sentences:
        word_tok = nltk.word_tokenize(sentence)
        stem = [stemmer.stem(word) for word in word_tok if word not in stopwords if len(word) > 2]
        sent_stem.append(stem)
    print('{}: {} ----- processed.'.format(i+1, text_id))

print("\n총 문장 개수 =", len(sent_stem))
print(sent_stem[0])

# 단어사전
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sent_stem)

# 단어사전
word2idx = tokenizer.word_index
word2idx['<PAD>'] = 0
idx2word = {v:k for k, v in word2idx.items()}

print("사전 크기 =", len(word2idx))

# 문장을 단어의 인덱스로 표현
sent_idx = tokenizer.texts_to_sequences(sent_stem)
print(sent_idx[0])

# trigram
x_train = []
y_train = []
for sent in sent_idx:
    if len(sent) < 3:
        continue

    for a, b, c in nltk.trigrams(sent):
        x_train.append(b)
        x_train.append(b)

        y_train.append(a)
        y_train.append(c)

x_train = np.array(x_train).reshape(-1, 1)
y_train = np.array(y_train).reshape(-1, 1)

x_train.shape, y_train.shape

VOC_SIZE = len(word2idx)
EMB_SIZE = 32

x_input = Input(batch_shape=(None, 1))
x_emb = Embedding(VOC_SIZE, EMB_SIZE, name='emb')(x_input)
y_output = Dense(VOC_SIZE, activation='softmax')(x_emb)

model = Model(x_input, y_output)
model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam')
model.summary()

# word --> word2vec을 확인하기 위한 모델
model_vec = Model(x_input, x_emb)

hist = model.fit(x_train, y_train, batch_size=20480, epochs=1)

def get_word2vec(word):
    stem_word = stemmer.stem(word)
    if stem_word not in word2idx:
        print('{}가 없습니다.'.format(word))
        return
    
    word2vec = model_vec.predict(np.array(word2idx[stem_word]).reshape(1,1))[0]
    return word2vec

father = get_word2vec('father')
mother = get_word2vec('mother')
doctor = get_word2vec('doctor')

print(father)

cosine_similarity(father, mother)

cosine_similarity(father, doctor)

W = model.get_layer('emb').get_weights()[0]
W.shape

