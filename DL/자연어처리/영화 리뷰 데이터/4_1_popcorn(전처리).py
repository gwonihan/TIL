# -*- coding: utf-8 -*-
"""4-1.popcorn(전처리).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-f6S9dk1ZI5It7mUzXFX_3Y4r-Kei50a
"""

import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import nltk
import re
from nltk.stem import PorterStemmer
import pickle
import numpy as np

nltk.download('punkt')
nltk.download('stopwords')

# 학습 데이터를 읽어온다.
DATA_PATH = '/content/drive/My Drive/Colab Notebooks/data/'

train_data = pd.read_csv(DATA_PATH + 'labeledTrainData.tsv', header=0, sep='\t', quoting=3)
train_data['review'][0]

# Pre-processing
stemmer = PorterStemmer()
stopwords = nltk.corpus.stopwords.words('english')

clean_text = []
for review in train_data['review']:
    # 1. 영문자와 숫자만 사용한다. 그 이외의 문자는 공백 문자로 대체한다.
    review = review.replace('<br />', ' ')       # <br> --> space
    review = review.replace('\'', '')            # dont't --> dont
    review = re.sub("[^a-zA-Z]", ' ', review)    # 영문자만 사용

    tmp = []
    for word in nltk.word_tokenize(review):
        # 2. 불용어 처리
        # 'e.g' 같은 경우는 'e g'로 처리된 상태임. 한글자 짜리 word는 버림.
        if len(word.lower()) > 1 and word.lower() not in stopwords:
            # 3. Stemming
            tmp.append(stemmer.stem(word.lower()))
    clean_text.append(' '.join(tmp))

clean_text[0]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(clean_text)
text_sequences = tokenizer.texts_to_sequences(clean_text)

print(text_sequences[0])

word2idx = tokenizer.word_index   # idx는 1부터 부여됨.
word2idx["<PAD>"] = 0
print(word2idx)

print("전체 단어 개수:", len(word2idx))

MAX_SEQ_LENGTH = 174  # 한 문장의 최대 길이
train_inputs = pad_sequences(text_sequences,
                             maxlen=MAX_SEQ_LENGTH,
                             padding='post')
print('Shape of train data: ', train_inputs.shape)

train_labels = np.array(train_data['sentiment'])
print('Shape of label tensor: ', train_labels.shape)

# 학습 데이터를 저장해 둔다.
with open(DATA_PATH + 'popcorn.pkl', 'wb') as f:
    pickle.dump([clean_text, train_inputs, train_labels, word2idx], f, pickle.DEFAULT_PROTOCOL)

