# -*- coding: utf-8 -*-
"""8-8.KorSTS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yaDUw0VBidWBW6B02_MyCPLCTJwmw1GJ
"""

!pip install transformers

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import transformers
from transformers import BertTokenizer, TFBertModel
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tqdm.auto import tqdm
from tensorflow.keras.callbacks import ModelCheckpoint
import os

transformers.logging.set_verbosity_error()

MAX_LEN = 28 * 2

# 데이터를 읽어온다
DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/data/KorSTS/'
train_data = pd.read_csv(DATA_PATH + 'sts-train.tsv', header=0, sep='\t', quoting = 3)[:100]
test_data = pd.read_csv(DATA_PATH + 'sts-test.tsv', header=0, sep='\t', quoting = 3)[:20]

print("Total # dataset: train - {}, test - {}".format(len(train_data), len(test_data)))
train_data.head()

# DATA_PATH 아래에 모델 파라메터를 저장할 파일을 설정한다.
ckpt_path = DATA_PATH + 'korsts_weights.h5'

# Create a callback that saves the model's weights, every epochs
cp_callback = ModelCheckpoint(filepath=ckpt_path, 
                              save_weights_only=True, 
                              verbose=1,
                              save_freq=1)

tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased", cache_dir='bert_ckpt', do_lower_case=False)
word2idx = tokenizer.vocab
idx2word = {v:k for k, v in word2idx.items()}

# Bert Tokenizer
def bert_tokenizer_v2(sent1, sent2):
    
    encoded_dict = tokenizer.encode_plus(
        text = sent1,
        text_pair = sent2,
        add_special_tokens = True,      # Add '[CLS]' and '[SEP]'
        max_length = MAX_LEN,           # Pad & truncate all sentences.
        padding = 'max_length',
        # pad_to_max_length = True,
        return_attention_mask = True,   # Construct attn. masks.
        truncation = True
    )
    
    input_id = encoded_dict['input_ids']
    attention_mask = encoded_dict['attention_mask'] # And its attention mask (simply differentiates padding from non-padding).
    token_type_id = encoded_dict['token_type_ids']  # differentiate two sentences
    
    return input_id, attention_mask, token_type_id

def build_data(sent1, sent2):
    x_ids = []
    x_msk = []
    x_typ = []

    for s1, s2 in tqdm(zip(sent1, sent2), total=len(sent1)):
        input_id, attention_mask, token_type_id = bert_tokenizer_v2(s1, s2)

        x_ids.append(input_id)
        x_msk.append(attention_mask)
        x_typ.append(token_type_id)
    
    x_ids = np.array(x_ids, dtype=int)
    x_msk = np.array(x_msk, dtype=int)
    x_typ = np.array(x_typ, dtype=int)
    # pdb.set_trace()

    return x_ids, x_msk, x_typ

x_train1 = train_data['sentence1']
x_train2 = train_data['sentence2']
y_train = train_data['score']

x_test1 = test_data['sentence1']
x_test2 = test_data['sentence2']
y_test = test_data['score']

x_train_ids, x_train_msk, x_train_typ = build_data(x_train1, x_train2)
x_test_ids, x_test_msk, x_test_typ = build_data(x_test1, x_test2)

y_train = np.array(y_train).reshape(-1, 1)
y_test = np.array(y_test).reshape(-1, 1)

x_train_ids.shape, y_train.shape, x_test_ids.shape, y_test.shape

# custom metrics
class pearson_corr(tf.keras.metrics.Metric):
    def __init__(self, name="pearson_corr", **kwargs):
        super(pearson_corr, self).__init__(name=name, **kwargs)
        self.y_true_list = []
        self.y_pred_list = []

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.reshape(y_true, shape=[-1])
        y_pred = tf.reshape(y_pred, shape=[-1])
        self.y_true_list.append(y_true)
        self.y_pred_list.append(y_pred)

    def result(self):
        y_true = tf.concat(self.y_true_list, -1)
        y_pred = tf.concat(self.y_pred_list, -1)
        corr = self.pearson(y_true, y_pred)
        return corr

    def reset_state(self):
        self.y_true_list = []
        self.y_pred_list = []
        
    def pearson(self, true, pred):
        m_true = true - tf.reduce_mean(true)
        m_pred = pred - tf.reduce_mean(pred)

        bunja = tf.reduce_sum(tf.multiply(m_true, m_pred))
        bunmo = tf.sqrt(tf.multiply(tf.reduce_sum(tf.square(m_true)), tf.reduce_sum(tf.square(m_pred)))) + 1e-12
        return bunja / bunmo

bert_model = TFBertModel.from_pretrained("bert-base-multilingual-cased", cache_dir='bert_ckpt')
bert_model.summary() # bert_model을 확인한다. trainable params = 177,853,440

# BERT 입력
# ---------
x_input_ids = Input(batch_shape = (None, MAX_LEN), dtype = tf.int32)
x_input_msk = Input(batch_shape = (None, MAX_LEN), dtype = tf.int32)
x_input_typ = Input(batch_shape = (None, MAX_LEN), dtype = tf.int32)

# BERT 출력
# ---------
output_bert = bert_model([x_input_ids, x_input_msk, x_input_typ])[1]

# Downstream task : 텍스트 유사도 (KoreSTS 데이터)
# ----------------------------------------------
y_output = Dense(1)(output_bert)
model = Model([x_input_ids, x_input_msk, x_input_typ], y_output)
model.compile(loss = 'mean_squared_error', 
              metrics=[pearson_corr()],
              optimizer = Adam(learning_rate = 0.001),
              run_eagerly = True)

model.summary()

if os.path.exists(ckpt_path):
    model.load_weights(ckpt_path)
    print('학습된 weight가 적용됐습니다.')

x_train = [x_train_ids, x_train_msk, x_train_typ]
x_test = [x_test_ids, x_test_msk, x_test_typ]

hist = model.fit(x_train, y_train, 
                 validation_data = (x_test, y_test), 
                 epochs=10, 
                 batch_size=20,
                 callbacks=[cp_callback])

# loss 확인
plt.plot(hist.history['loss'], label='train')
plt.plot(hist.history['val_loss'], label='test')
plt.legend()
plt.show()

# corr 확인
plt.plot(hist.history['pearson_corr'], label='train')
plt.plot(hist.history['val_pearson_corr'], label='test')
plt.title('correlation')
plt.legend()
plt.show()

# test 데이터의 score를 추정한다.
y_pred = model.predict(x_test)

# 추정 결과를 산포도로 확인한다.
corr = np.corrcoef(y_test.reshape(-1,), y_pred.reshape(-1,))[0,1]

plt.scatter(y_test, y_pred, marker='o', s=10, c='red')
plt.xlabel('y_test')
plt.ylabel('y_pred')
plt.title('corr =' + str(corr.round(4)))
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.show()

