# -*- coding: utf-8 -*-
"""8-14.naver_movie(kogpt2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BoK7AHLHQuHtKu_Eg2IZpKVWL25sjNR8
"""

# KoGPT2를 이용한 네이버 영화 감성분석
# 참고 : https://github.com/SKT-AI/KoGPT2
!pip install --upgrade mxnet>=1.6.0
!pip install gluonnlp
!pip install transformers
!pip install sentencepiece
!pip install wget

import gluonnlp as nlp
from gluonnlp.data import SentencepieceTokenizer, SentencepieceDetokenizer
from transformers import TFGPT2LMHeadModel
import tensorflow as tf

import pandas as pd
import numpy as np
from tensorflow.keras.layers import Dense, Input, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
import re
from tqdm import tqdm
import matplotlib.pyplot as plt

import wget
import zipfile

wget.download('https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip')

with zipfile.ZipFile('gpt_ckpt.zip') as z:
    z.extractall()

MODEL_PATH = '/content/gpt_ckpt'
TOKENIZER_PATH = '/content/gpt_ckpt/gpt2_kor_tokenizer.spiece'

DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/data/'
df = pd.read_csv(DATA_PATH + 'naver_movie/ratings.txt', header=0, delimiter='\t', quoting=3)
df = df.dropna()
df.head()

# "\d+"는 숫자 1개 이상을 의미함. 모든 숫자를 공백으로 치환
df['document'] = df['document'].apply(lambda x: re.sub(r"\d+", " ", x))
df.drop('id', axis = 1, inplace = True)

document = list(df['document'])
label = list(df['label'])
dx_train, dx_test, dy_train, dy_test = train_test_split(document, label, test_size=0.2)

tokenizer = SentencepieceTokenizer(TOKENIZER_PATH, num_best=0, alpha=0)
detokenizer = SentencepieceDetokenizer(TOKENIZER_PATH)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,
                                               mask_token = None,
                                               sep_token = None,
                                               cls_token = None,
                                               unknown_token = '<unk>',
                                               padding_token = '<pad>',
                                               bos_token = '<s>',
                                               eos_token = '</s>')
# vocab --> Vocab(size=50000, unk="<unk>", reserved="['<pad>', '<s>', '</s>']")

MAX_LEN = 60

def build_data(x_data, y_label):
    data_sents = []
    data_labels = []

    for sent, label in zip(x_data, y_label):
        tokenized_text = vocab[tokenizer(sent)]

        tokens = [vocab[vocab.bos_token]]   # 시작 = <s>
        tokens += pad_sequences([tokenized_text], 
                                MAX_LEN, 
                                value=vocab[vocab.padding_token], 
                                padding='post').tolist()[0] 
        tokens += [vocab[vocab.eos_token]]  # 끝 = </s>

        data_sents.append(tokens)
        data_labels.append(label)

    return np.array(data_sents, dtype=np.int64), np.array(data_labels, dtype=np.int64).reshape(-1, 1)

# 시험용으로 100개씩만 사용한다.
x_train, y_train = build_data(dx_train[:100], dy_train[:100])
x_test, y_test = build_data(dx_test[:100], dy_test[:100])

x_train.shape, y_train.shape, x_test.shape, y_test.shape

x_test[0]

len(x_test[0])

print(len(vocab))
print(vocab.padding_token, ':', vocab[vocab.padding_token])
print(vocab.bos_token, ': ', vocab[vocab.bos_token])
print(vocab.eos_token, ': ', vocab[vocab.eos_token])
print(vocab.unknown_token, ': ', vocab[vocab.unknown_token])

word2idx = {k:v for k, v in vocab.token_to_idx.items()}
idx2word = {v:k for k, v in word2idx.items()}
idx2word[5000]

# 참고 : https://nlp.gluon.ai/api/modules/data.html
sub_word = tokenizer('나는 자연어처리를 공부하고 있다')
print(sub_word)

sent_idx = vocab[sub_word]
print(sent_idx)

print(detokenizer(sub_word))

print([idx2word[i] for i in x_test[0]])

gpt_model = TFGPT2LMHeadModel.from_pretrained(MODEL_PATH)
gpt_model.summary()

# GPT2 입력
# ---------
x_input = Input(batch_shape = (None, MAX_LEN + 2), dtype = tf.int32)  # <s>와 </s> 2개 포함

# GPT2 출력
# ---------
# for classification
# output_gpt[0]           --> <KerasTensor: shape=(None, 62, 50000) dtype=float32
# output_gpt[0][:, -1, :] --> <KerasTensor: shape=(None, 50000) dtype=float32
output_gpt = gpt_model(x_input)[0][:, -1, :]

# Downstream task : 네이버 영화 감성분석
# -------------------------------------
y_output = Dense(1, activation = 'sigmoid')(output_gpt)
model = Model(x_input, y_output)
model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001))
model.summary()

hist = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=3, batch_size=1024)

# 1/1 [==============================] - 14s 14s/step - loss: 0.7251 - val_loss: 70.8553
# Epoch 2/3
# 1/1 [==============================] - 1s 1s/step - loss: 69.2513 - val_loss: 26.5999
# Epoch 3/3
# 1/1 [==============================] - 1s 1s/step - loss: 25.5047 - val_loss: 21.4767

# Loss history를 그린다
plt.plot(hist.history['loss'], label='Train loss')
plt.plot(hist.history['val_loss'], label = 'Test loss')
plt.legend()
plt.title("Loss history")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

# 시험 데이터로 학습 성능을 평가한다
pred = model.predict(x_test)
y_pred = np.where(pred > 0.5, 1, 0)
accuracy = (y_pred == y_test).mean()
print("\nAccuracy = %.2f %s" % (accuracy * 100, '%'))