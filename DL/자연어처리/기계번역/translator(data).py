# -*- coding: utf-8 -*-
"""7-7.translator(data).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v_R8V4MFuBt9xZTuHNqWAmQStJDsK0aR
"""

!pip install sentencepiece

# Seq2Seq-Attention Machine Translator : 학습 데이터 모듈
# Google의 Sentencepiece를 이용해서 학습 데이터를 생성한다.
#
# 저작자: 2021.08.02, 조성현 (blog.naver.com/chunjein)
# copyright: SNS 등에 공개할 때는 출처에 저작자를 명시해 주시기 바랍니다.
# -------------------------------------------------------------------
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import pandas as pd
import sentencepiece as spm
import re
import pickle
import seaborn as sns

# Commented out IPython magic to ensure Python compatibility.
# 작업 디렉토리를 변경한다.
# %cd '/content/drive/My Drive/Colab Notebooks'

# 데이터 파일을 읽어온다.
df = pd.read_csv('data/machine_trans.csv', header=0)
source, target = list(df['source']), list(df['target'])
df.head()

# 학습 데이터와 시험 데이터를 분리한다.
src_train, src_test, tar_train, tar_test = train_test_split(source, target, test_size=0.1, random_state=0)

src_train[0], tar_train[0]

# Google의 Sentencepiece를 이용해서 vocabulary를 생성한다.
# -----------------------------------------------------
templates= "--input={} \
            --pad_id=0 --pad_piece=<PAD>\
            --unk_id=1 --unk_piece=<UNK>\
            --bos_id=2 --bos_piece=<BOS>\
            --eos_id=3 --eos_piece=<EOS>\
            --model_prefix={} \
            --vocab_size={}"

# Sentencepice용 한글 (source) 사전을 만들기 위해 src_train + src_test를 저장해 둔다.
data_file = "data/mt_source.txt"
with open(data_file, 'w', encoding='utf-8') as f:
    for sent in src_train + src_test:
        f.write(sent + '\n')

SRC_VOCAB = 9000
model_prefix = "data/source_model"
params = templates.format(data_file, model_prefix, SRC_VOCAB)

spm.SentencePieceTrainer.Train(params)
sp_source = spm.SentencePieceProcessor()
sp_source.Load(model_prefix + '.model')

with open(model_prefix + '.vocab', encoding='utf-8') as f:
    vocab = [doc.strip().split('\t') for doc in f]

src_word2idx = {k:v for v, [k, _] in enumerate(vocab)}
src_idx2word = {v:k for v, [k, _] in enumerate(vocab)}

print([src_idx2word[i] for i in range(20)])

# Sentencepice용 영어 (target) 사전을 만들기 위해 tar_train + tar_test를 저장해 둔다.
data_file = "data/mt_target.txt"
with open(data_file, 'w', encoding='utf-8') as f:
    for sent in tar_train + tar_test:
        f.write(sent + '\n')

TAR_VOCAB = 4798
model_prefix = "data/target_model"
params = templates.format(data_file, model_prefix, TAR_VOCAB)

spm.SentencePieceTrainer.Train(params)
sp_target = spm.SentencePieceProcessor()
sp_target.Load(model_prefix + '.model')

with open(model_prefix + '.vocab', encoding='utf-8') as f:
    vocab = [doc.strip().split('\t') for doc in f]

tar_word2idx = {k:v for v, [k, _] in enumerate(vocab)}
tar_idx2word = {v:k for v, [k, _] in enumerate(vocab)}

print([tar_idx2word[i] for i in range(20)])

# 학습 데이터를 생성한다. (인코더 입력용, 디코더 입력용, 디코더 출력용)
MAX_LEN_SRC = 15
MAX_LEN_TAR = 20
enc_input = []
dec_input = []
dec_output = []

for src, tar in zip(src_train, tar_train):
    # Encoder 입력
    enc_i = sp_source.encode_as_ids(src)
    enc_input.append(enc_i)

    # Decoder 입력, 출력
    dec_i = [sp_target.bos_id()]   # <BOS>에서 시작함
    dec_o = []
    for ans in sp_target.encode_as_ids(tar):
        dec_i.append(ans)
        dec_o.append(ans)
    dec_o.append(sp_target.eos_id())   # Encoder 출력은 <EOS>로 끝남.        
    
    # dec_o는 <EOS>가 마지막에 들어있다. 나중에 pad_sequences()에서 <EOS>가
    # 잘려 나가지 않도록 MAX_LEN 위치에 <EOS>를 넣어준다.
    if len(dec_o) > MAX_LEN_TAR:
        dec_o[MAX_LEN_TAR] = sp_target.eos_id()
        
    dec_input.append(dec_i)
    dec_output.append(dec_o)

# 문장 길이 분포를 파악한다.
sns.displot([len(s) for s in dec_input])

# 각 문장의 길이를 맞추고 남는 부분에 padding을 삽입한다.
enc_input = pad_sequences(enc_input, maxlen=MAX_LEN_SRC, value = sp_source.pad_id(), padding='post', truncating='post')
dec_input = pad_sequences(dec_input, maxlen=MAX_LEN_TAR, value = sp_target.pad_id(), padding='post', truncating='post')
dec_output = pad_sequences(dec_output, maxlen=MAX_LEN_TAR, value = sp_target.pad_id(), padding='post', truncating='post')

# 사전과 학습 데이터를 저장한다.
with open('data/mt_voc.pkl', 'wb') as f:
    pickle.dump([src_word2idx, src_idx2word, tar_word2idx, tar_idx2word], f, pickle.HIGHEST_PROTOCOL)

# BLEU 평가를 위해 que_test와 ans_test를 저장해 둔다.
with open('data/mt_train.pkl', 'wb') as f:
    pickle.dump([enc_input, dec_input, dec_output, src_test, tar_test], f, pickle.HIGHEST_PROTOCOL)

enc_input[0]

dec_input[0]

dec_output[0]

[(s, t) for s, t in zip(src_train[:10], tar_train[:10])]

[(s, t) for s, t in zip(src_test[:10], tar_test[:10])]

