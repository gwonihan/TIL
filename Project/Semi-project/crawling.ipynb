{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "news = []\n",
        "for i in range(1,4):\n",
        "  url = 'https://news.daum.net/breakingnews/sports/worldsoccer?page=' + str(i)\n",
        "  res = requests.get(url)\n",
        "\n",
        "  soup = bs(res.text, 'lxml')\n",
        "  ul = soup.find(\"ul\",{\"class\":\"list_news2 list_allnews\"}).findAll(\"li\")\n",
        "\n",
        "  for li in ul:\n",
        "        data = li.find(\"a\",{\"class\":\"link_txt\"})\n",
        "        news.append({\n",
        "          'title': data.text,\n",
        "          'topic_idx': 5\n",
        "              })\n",
        "\n",
        "# 중복제거\n",
        "news = list(map(dict, set(tuple(sorted(d.items())) for d in news)))\n",
        "\n",
        "dataframe = pd.DataFrame(news)\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/files/news/kor/craw_data/sport6.csv\"\n",
        "dataframe.to_csv(PATH, index=False )"
      ],
      "metadata": {
        "id": "zlg1SKgv4nDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수집한 데이터를 하나의 csv 파일로 만든다.\n",
        "\n",
        "import os\n",
        "forders = os.listdir('/content/drive/MyDrive/files/news/eng/craw_data')\n",
        "print(forders)\n",
        "\n",
        "df_all = pd.DataFrame()\n",
        "for i in range(0,len(forders)):\n",
        "    if forders[i].split('.')[1] == 'csv':\n",
        "        file = '/content/drive/MyDrive/files/news/eng/craw_data/'+forders[i]\n",
        "        df= pd.read_csv(file,encoding='utf-8') \n",
        "        df_all = pd.concat([df_all, df])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExQ5yh5KVy3r",
        "outputId": "787e76da-90f4-4706-b429-cf6d9eb307f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['IT1.csv', 'IT2.csv', 'economy1.csv', 'world1.csv', 'economy2.csv', 'politics.csv', 'sports1.csv', 'corona.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# csv 파일로 저장\n",
        "PATH = \"/content/drive/MyDrive/files/news/eng/craw_data/eng_data.csv\"\n",
        "df_all.to_csv(PATH, index=False )"
      ],
      "metadata": {
        "id": "XJZDxqyQWOy7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}