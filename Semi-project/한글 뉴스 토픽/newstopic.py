# -*- coding: utf-8 -*-
"""NewsTopic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13e87hS3x-fYmOm-y77d0Gk3g_YSDaik6

# 한국어 뉴스 토픽
"""
# 참조 유튜브 채널
# https://www.youtube.com/channel/UCLR3sD0KB_dWpvcsrLP0aUg


# 폰트
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic')

"""## 1.데이터 탐색"""

!pip install transformers

import numpy as np
import pandas as pd
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.train import latest_checkpoint
import re
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic') 
import os
import seaborn as sns

# 학습, 예측 데이터셋을 불러옵니다
# train, test
DATA_PATH = '/content/drive/MyDrive/files/news/'
train = pd.read_csv(DATA_PATH + "kor/train_data.csv")
topic = pd.read_csv(DATA_PATH + "kor/topic_dict.csv")
test = pd.read_csv(DATA_PATH + "kor/test_data.csv")
train.shape, test.shape

train.head()

test.head()

# 토픽을 불러온다
topic = pd.read_csv(DATA_PATH + "kor/topic_dict.csv")
topic

train.head()

topic.head()

test.head()

train.info()

train.isnull().sum()

topic.info()

topic.to_numpy()

test.info()

test.isnull().sum()

"""## 2.데이터 전처리"""

# 정규표현식 불러오기
import re

def preprocessing(text):
  # 개행문자 제거
  text = re.sub('\\\\n', ' ', text)

  # 한글, 영문만 남기고 모두 제거
  text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]',' ', text)
  # 중복으로 생성된 공백값을 제거
  text = re.sub('[\s]+', ' ', text)
  # 영어롤 소문자로
  text = text.lower()
  return text

# map을 통해 전처리 일괄 적용
train["title"] = train["title"].map(preprocessing)
test["title"] = test["title"].map(preprocessing)

# 불용어 제거
def remove_stopwords(text):
  tokens = text.split(' ')
  stops = ['합니다', '하는', '할', '하고', '한다', '그리고', '입니다', '그', '등', '이런', '및', '제', '더']
  meaningful_words = [w for w in tokens if not w in stops]
  return ' '.join(meaningful_words)
  
# map을 통해 불용어 일괄 제거
train["title"] = train["title"].map(remove_stopwords)
test["title"] = test["title"].map(remove_stopwords)
print(train.head())
print(test.head())

# 정답값으로 사용할 topic_idx를 변수에 담아 재사용 한다
label_name = "topic_idx"

x_train = train['title']
x_test = test['title']
y_train = train['topic_idx']

print(y_train.shape)

y_train.head()

train['topic_idx']

# 타이틀 글자 최대 길이 구하기
# 문자 타입, 리스트 변환
train_set = pd.Series(train['title'].tolist()).astype(str)
print(train_set.head())

# 글자 개수 세기
train_word_counts = train_set.apply(lambda x:len(x))

print(max(train_word_counts))

"""## 3.정답값 빈도수"""

# 전처리를 위해 concat으로 데이터 병합
raw = pd.concat([train, test])
raw.shape

train.shape[0] + test.shape[0]

raw.head()

raw.tail()

raw.shape

# df = train + test + topic
df = raw.merge(topic, on='topic_idx', how="left")
df.shape

df.head()

"""## 4.정답값 빈도수"""

# test는 결측치로 되어 있기 때문에 빈도수에 포함되지 않는다
# topic_idx 의 빈도수를 구한다
df["topic_idx"].value_counts()

# topic의 빈도수를 구한다
df["topic"].value_counts()

# train2 로 빈도수를 구했지만 test 데이터는 topic이 결측치라 포함되지 않는다.
# order value_counts 기준으로 정렬을 해준다.
sns.countplot(data=df, x='topic', order=df["topic"].value_counts().index)

"""## 4.문자길이"""

# apply, lambda를 통해 문자ㅡ 단어 빈도수 파생변수 만들기
# df["len"]
# df["word_count"]
# df["unique_word_count"]
df["len"] = df["title"].apply(lambda x: len(x))
df["word_count"] = df["title"].apply(lambda x: len(x.split()))
df["unique_word_count"] = df["title"].apply(lambda x: len(set(x.split())))
df

# 서브플롯을 통해 "len", "word_count", "unique_word_count"의 histplot을 시각화 한다
sns.histplot(data=df, x="len")

# 확인해본 결과 특별히 눈에 띄는 값은 없음
df[["len", "word_count", "unique_word_count"]].hist(bins=30)
plt.show()

# 통계값을 확인한다.
df[["len", "word_count", "unique_word_count"]].describe()

# 문장길이
# displot 으로 topic 별 "len" 의 histplot 시각화 하기
# data=df, x="len", kind="hist", hue="topic", col="topic", col_wrap=2, aspect=5, height=2
sns.displot(data=df, x="len",hue="topic", col="topic", col_wrap=2, aspect=5, height=2)

# 단어 수
# displot 으로 topic 별 "word_count"의 histplot 시각화 하기
sns.displot(data=df, x="word_count",hue="topic",col="topic", col_wrap=2, height=2, aspect=5)

# 중복 제거 단어 수
# displot 으로 topic 별 "unique_word_count" 의 histplot 시각화 하기
sns.displot(data=df, x="unique_word_count", hue="topic", col="topic", col_wrap=2, height=2, aspect=5)

# heatmap을 통한 "len", "word_count", "unique_word_count" 시각화
sns.heatmap(df[["len", "word_count", "unique_word_count"]], cmap="Blues")

"""## 6.워드클라우드"""

!pip install wordcloud

from wordcloud import WordCloud

def display_word_cloud(data, width=1200,height=500):
  word_draw = WordCloud(
      font_path="NanumBarunGothic",
      width=width, height=height,
      background_color="white",
      random_state=42
  )
  word_draw.generate(data)

  plt.figure(figsize=(15,7))
  plt.imshow(word_draw)
  plt.axis("off")
  plt.show()

# join()을 이용하여 변수 title 리스트에서 문자열로 변환해 준다.
# content
content = " ".join(df["title"].tolist())
content[:100]

len(content)

# content를 위해서 만든 함수인 display_word_cloud로 워드클라우드를 시각화 한다.
display_word_cloud(content)

"""### 특정 토픽만 워드클라우드 시각화"""

# topic의 unique 값만 보기
df["topic"].unique()

# df.loc로 특정 토픽만 가져와서 join으로 문자열을 연결한다.
content = " ".join(df.loc[df["topic"] == "스포츠","title"].tolist())
content[:100]

# content를 위해서 만든 함수인 display_word_cloud로 워드클라우드를 시각화 한다.
display_word_cloud(content)

"""## 5.모델생성(BERT)"""

